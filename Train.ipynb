{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Edible Plants\n",
    "## w251 Final Project\n",
    "## Scott Xu, Divya Babu, Aaron Olson\n",
    "\n",
    "The Intent of this final project is to develop an image recognition program that can accurately identify edible and/or poisonous plants in the wild. This endeavor has been attempted by several apps and other programs - however all of these realize an edge architecture that relies on a remote server connection in order to upload the file and run through the model. \n",
    "\n",
    "This paper explores the difference performance options in order to arrive at the best performing model. We then work to reduce the model size in order to fit on an edge device for real time diagnosis. \n",
    "\n",
    "In order to get a baseline model for image recognition, we used a transfer learning technique where the model weights and architecture of ResNet50 was applied. ResNet50 was chosen for its performance as well as its size. Training on the volume of images for the duration that ResNet50 was done would not be reasonable - therefore we have used this baseline model to improve the baseline prediction. On top of this we explore different model architectures in order to define which architecture performs the best.\n",
    "\n",
    "In order to get the best performing model we needed to remember to balance model performance with edge device performance. In the case of poisonous plants the consequences of a bad prediction can be high - however the utility of an app that takes 60 min to make a prediction is impacted. Therefore at the end of this notebook we examine the relationship with building the model on a virtual machine (for training) vs inference on the edge device (time to predict vs accuracy). \n",
    "\n",
    "We begin by examining the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert cells with example pictures / labels and a few details about training size / etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50 baseline model\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "HEIGHT = 300\n",
    "WIDTH = 300\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(HEIGHT, WIDTH, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "\n",
    "With the baseline model established, we understand that there will likely be a difference in the cleanliness of the images taken for the training dataset, vs the images taken in the field when a user wants to succesfully identify a plant. We therefore utilize image augmentation, to achieve a couple of tasks: (1) affect the image quality, orientation, etc in order to make the model more versatile (2) create more training images in order to train the model. \n",
    "\n",
    "Below we explore different effects of image augmentation and show below the effects of model performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4220 images belonging to 37 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image augmentation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "TRAIN_DIR = \"train_edible\"\n",
    "result = [y for x in os.walk(TRAIN_DIR) for y in glob(os.path.join(x[0], '*.jpg'))]\n",
    "classes = list(set([y.split(\"/\")[-2] for y in result]))\n",
    "HEIGHT = 300\n",
    "WIDTH = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen =  ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input,\n",
    "      rotation_range=90,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      height_shift_range=0.5,\n",
    "      width_shift_range=0.5\n",
    "    )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DIR, \n",
    "                                                    target_size=(HEIGHT, WIDTH), \n",
    "                                                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture - Predicting Plant Class\n",
    "\n",
    "Building on the baseline model, we have explored adding different layers (size, type, etc) in order to produce the 'best' performing model. See above and later discussions as to how we define 'best' model. For this paper we explored both accurately predicting the class of the plant in an image - which would help users understand more information about the specific plant they have taken the picture of, as well as predicting whether a plant is poisonous or not without the context of exact plant type. \n",
    "\n",
    "The original intent of this paper was to predict the plant class whereby the metadata associated with that prediction would yield the label of poisonous or edible - however we also chose to explore whether deep learning could accurately predict the binary choice of poisonous or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    for fc in fc_layers:\n",
    "        # Can look here if adding different types of layers has an effect\n",
    "        # Also explore differences in changing activation function\n",
    "        # Can also iterate on droupout amount\n",
    "        x = Dense(fc, activation='relu')(x) \n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    # New softmax layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) \n",
    "    \n",
    "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return finetune_model\n",
    "\n",
    "# Can change the model architecture here\n",
    "FC_LAYERS = [128,64,32,16]\n",
    "dropout = 0.3\n",
    "\n",
    "finetune_model = build_finetune_model(base_model, \n",
    "                                      dropout=dropout, \n",
    "                                      fc_layers=FC_LAYERS, \n",
    "                                      num_classes=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 3.2889 - acc: 0.1154\n",
      "\n",
      "Epoch 00001: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 120s 154ms/step - loss: 3.1407 - acc: 0.1662\n",
      "\n",
      "Epoch 00002: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 120s 153ms/step - loss: 3.0145 - acc: 0.2026\n",
      "\n",
      "Epoch 00003: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 120s 154ms/step - loss: 2.9042 - acc: 0.2331\n",
      "\n",
      "Epoch 00004: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 120s 154ms/step - loss: 2.8408 - acc: 0.2521\n",
      "\n",
      "Epoch 00005: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 120s 153ms/step - loss: 2.7598 - acc: 0.2763\n",
      "\n",
      "Epoch 00006: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 109s 140ms/step - loss: 2.6939 - acc: 0.2933\n",
      "\n",
      "Epoch 00007: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 106s 135ms/step - loss: 2.6323 - acc: 0.3129\n",
      "\n",
      "Epoch 00008: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.6104 - acc: 0.3246\n",
      "\n",
      "Epoch 00009: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.5687 - acc: 0.3359\n",
      "\n",
      "Epoch 00010: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.5315 - acc: 0.3449\n",
      "\n",
      "Epoch 00011: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.4995 - acc: 0.3543\n",
      "\n",
      "Epoch 00012: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.4804 - acc: 0.3580\n",
      "\n",
      "Epoch 00013: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.4569 - acc: 0.3657\n",
      "\n",
      "Epoch 00014: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.4322 - acc: 0.3693\n",
      "\n",
      "Epoch 00015: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.4091 - acc: 0.3759\n",
      "\n",
      "Epoch 00016: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 106s 135ms/step - loss: 2.3943 - acc: 0.3791\n",
      "\n",
      "Epoch 00017: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.3764 - acc: 0.3788\n",
      "\n",
      "Epoch 00018: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.3589 - acc: 0.3832\n",
      "\n",
      "Epoch 00019: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 106s 136ms/step - loss: 2.3490 - acc: 0.3885\n",
      "\n",
      "Epoch 00020: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 106s 135ms/step - loss: 2.3427 - acc: 0.3890\n",
      "\n",
      "Epoch 00021: saving model to checkpoint/edible_default_model_weights.h5\n",
      "Epoch 22/100\n",
      "739/781 [===========================>..] - ETA: 5s - loss: 2.3175 - acc: 0.3940"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy   \n",
    "    \n",
    "# For the baseline model will use 100 epochs and 15000 images to test model performance\n",
    "# Will then use 'optimized' model parameters to train for longer time and explore\n",
    "# Size vs Accuracy for edge compute purposes\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "num_train_images = 50000\n",
    "\n",
    "adam = Adam(lr=0.00001)\n",
    "# Can look into whether \n",
    "finetune_model.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Checkpoin is overwritten at each epoch - can look at line below where datetime is used to create time based file names\n",
    "filepath=\"checkpoint/\" + \"edible_default\" + \"_model_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
    "# log_dir = \"C:\\\\Users\\\\AOlson\\\\Documents\\\\UC Berkeley MIDS\\\\w251_Scaling\\\\Final Project\\\\Data\\\\log_dir\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = finetune_model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
    "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
    "                                       shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "numpy.savetxt('edible_loss_history.txt', numpy.array(history.history['loss']), delimiter = ',')\n",
    "numpy.savetxt('edible_acc_history.txt', numpy.array(history.history['acc']), delimiter = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
